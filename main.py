import os
import glob
import random
import numpy as np
from collections import defaultdict
from sklearn.model_selection import train_test_split
from dataset import VoiceDataset
from train import train_model
from evaluate import evaluate_model, plot_evaluation_results
from utils import convert_common_voice_to_wav
import torch
import shutil

# ======== –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ========
TSV_PATH = r"C:\Users\–õ–∏–∑–∞\Desktop\–î–∏–ø–ª–æ–º\voice-auth\en\train.tsv"
AUDIO_DIR = r"C:\Users\–õ–∏–∑–∞\Desktop\–î–∏–ø–ª–æ–º\voice-auth\en\clips"
PROCESSED_DIR = "full_data"
MIN_FILES_PER_USER = 8  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
TEST_SIZE = 0.2
VAL_SIZE = 0.25
BATCH_SIZE = 64      # –£–º–µ–Ω—å—à–∏–ª–∏ batch size –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
EPOCHS = 100         # –£–≤–µ–ª–∏—á–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
N_MFCC = 40          # –ë–æ–ª—å—à–µ MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
MAX_LEN = 256        # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# ======== –ü–û–î–ì–û–¢–û–í–ö–ê ========
print("\n" + "="*80)
print("IMPROVED VOICE AUTHENTICATION SYSTEM - FULL DATASET PROCESSING")
print("="*80 + "\n")

# –û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
if os.path.exists(PROCESSED_DIR):
    print(f"‚ö†Ô∏è Warning: Output directory {PROCESSED_DIR} already exists")
    # shutil.rmtree(PROCESSED_DIR)  # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –æ—á–∏—Å—Ç–∫–∏

# ======== –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø ========
print("\n===== AUDIO CONVERSION =====")
converted_files = convert_common_voice_to_wav(
    tsv_path=TSV_PATH,
    input_dir=AUDIO_DIR,
    output_dir=PROCESSED_DIR
)

if converted_files == 0:
    print("‚ùå Conversion failed. Exiting.")
    exit(1)

# ======== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ========
print("\n===== DATA PREPARATION =====")
files = glob.glob(os.path.join(PROCESSED_DIR, "user_*.wav"))
print(f"Found {len(files)} WAV files in {PROCESSED_DIR}")

if len(files) == 0:
    print("‚ùå No files found! Exiting.")
    exit(1)

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
user_files = defaultdict(list)
for f in files:
    filename = os.path.basename(f)
    parts = filename.split('_')
    if len(parts) >= 3:
        user_id = parts[1]
        user_files[user_id].append(f)

print(f"Total users: {len(user_files)}")

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∞–π–ª–æ–≤
valid_users = [user for user in user_files if len(user_files[user]) >= MIN_FILES_PER_USER]
print(f"Users with ‚â•{MIN_FILES_PER_USER} recordings: {len(valid_users)}")

if len(valid_users) == 0:
    print("‚ùå No valid users found! Exiting.")
    exit(1)

# –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
MAX_USERS = 100  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if len(valid_users) > MAX_USERS:
    valid_users = valid_users[:MAX_USERS]
    print(f"Limited to {MAX_USERS} users for this experiment")

# –°–æ–∑–¥–∞–Ω–∏–µ dataset
file_list = []
labels = []
label2id = {user: idx for idx, user in enumerate(valid_users)}
id2label = {idx: user for user, idx in label2id.items()}

for user in valid_users:
    for f in user_files[user]:
        file_list.append(f)
        labels.append(label2id[user])

print(f"Total files selected: {len(file_list)}")
print(f"Unique users: {len(valid_users)}")
print(f"Average files per user: {len(file_list) / len(valid_users):.1f}")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
print("\n===== DATA SPLITTING =====")
train_files, test_files, train_labels, test_labels = train_test_split(
    file_list, labels,
    test_size=TEST_SIZE,
    stratify=labels,
    random_state=42
)

train_files, val_files, train_labels, val_labels = train_test_split(
    train_files, train_labels,
    test_size=VAL_SIZE,
    stratify=train_labels,
    random_state=42
)

print(f"Train set: {len(train_files)} files")
print(f"Validation set: {len(val_files)} files") 
print(f"Test set: {len(test_files)} files")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
from collections import Counter
train_class_counts = Counter(train_labels)
print(f"Train class balance - Min: {min(train_class_counts.values())}, "
      f"Max: {max(train_class_counts.values())}, "
      f"Mean: {np.mean(list(train_class_counts.values())):.1f}")

# –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
print("\n===== DATASET CREATION =====")
train_ds = VoiceDataset(
    train_files, train_labels, 
    n_mfcc=N_MFCC, max_len=MAX_LEN,
    cache_dir="train_cache", 
    augment=True  # –í–∫–ª—é—á–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
)

val_ds = VoiceDataset(
    val_files, val_labels,
    n_mfcc=N_MFCC, max_len=MAX_LEN,
    cache_dir="val_cache",
    augment=False  # –ë–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
)

test_ds = VoiceDataset(
    test_files, test_labels,
    n_mfcc=N_MFCC, max_len=MAX_LEN,
    cache_dir="test_cache",
    augment=False  # –ë–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
)

print(f"Training dataset size: {len(train_ds)}")
print(f"Validation dataset size: {len(val_ds)}")
print(f"Test dataset size: {len(test_ds)}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
sample_input, sample_label = train_ds[0]
print(f"Input shape: {sample_input.shape}")
print(f"Feature dimension: {sample_input.shape[0]}")
print(f"Sequence length: {sample_input.shape[1]}")

# ======== –û–ë–£–ß–ï–ù–ò–ï ========
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\n===== TRAINING ON: {device.upper()} =====")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

model = train_model(
    train_ds,
    val_ds,
    num_classes=len(valid_users),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    device=device,
    save_path="improved_models"
)

# ======== –î–ï–¢–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê ========
print("\n===== DETAILED EVALUATION =====")
test_loader = torch.utils.data.DataLoader(
    test_ds,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=4 if device == "cuda" else 2,
    pin_memory=True
)

results = evaluate_model(model, test_loader, device=device, threshold_step=0.001)

if results is not None:
    print("\n===== RESULTS SUMMARY =====")
    print(f"üéØ EER (Equal Error Rate): {results['eer']:.4f} ({results['eer']*100:.2f}%)")
    print(f"üìä AUC (Area Under Curve): {results['auc']:.4f}")
    print(f"üé≤ Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"‚ùå FAR (False Accept Rate): {results['far']:.4f} ({results['far']*100:.2f}%)")
    print(f"üö´ FRR (False Reject Rate): {results['frr']:.4f} ({results['frr']*100:.2f}%)")
    print(f"üîç Min DCF: {results['min_dcf']:.4f}")
    print(f"üìè d-prime (separability): {results['d_prime']:.4f}")
    print(f"üìê Metric used: {results['metric_used']}")
    print(f"üîó Separability: {results['separability']:.4f}")
    
    print(f"\nüìà Genuine scores - Mean: {results['genuine_stats']['mean']:.4f}, "
          f"Std: {results['genuine_stats']['std']:.4f}")
    print(f"üìâ Impostor scores - Mean: {results['impostor_stats']['mean']:.4f}, "
          f"Std: {results['impostor_stats']['std']:.4f}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    try:
        plot_evaluation_results(results, "improved_evaluation_plots.png")
        print("üìä Evaluation plots saved to improved_evaluation_plots.png")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not create plots: {e}")

    # ======== –°–û–•–†–ê–ù–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò ========
    print("\n===== SAVING IMPROVED MODEL =====")
    model_path = "voice_auth_improved_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'label2id': label2id,
        'id2label': id2label,
        'config': {
            'n_mfcc': N_MFCC,
            'max_len': MAX_LEN,
            'output_dim': 512,
            'num_users': len(valid_users)
        },
        'results': results,
        'training_params': {
            'epochs': EPOCHS,
            'batch_size': BATCH_SIZE,
            'min_files_per_user': MIN_FILES_PER_USER
        }
    }, model_path)

    print(f"‚úÖ Improved model saved to {model_path}")
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n===== PERFORMANCE INTERPRETATION =====")
    if results['eer'] < 0.05:
        print("üü¢ EXCELLENT: EER < 5% - Production ready system")
    elif results['eer'] < 0.10:
        print("üü° GOOD: EER < 10% - Acceptable for most applications")
    elif results['eer'] < 0.20:
        print("üü† FAIR: EER < 20% - Needs improvement for security applications")
    else:
        print("üî¥ POOR: EER > 20% - Significant improvements needed")
    
    if results['auc'] > 0.95:
        print("üü¢ EXCELLENT: AUC > 95% - Very good discriminability")
    elif results['auc'] > 0.90:
        print("üü° GOOD: AUC > 90% - Good discriminability")
    elif results['auc'] > 0.80:
        print("üü† FAIR: AUC > 80% - Moderate discriminability")
    else:
        print("üî¥ POOR: AUC < 80% - Poor discriminability")

else:
    print("‚ùå Evaluation failed!")

print("\n" + "="*80)
print("IMPROVED PROCESSING COMPLETE!")
print("="*80)